{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(tf.add(40,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neu0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc411396f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load game env\n",
    "from gridworld import gameEnv\n",
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, H_SIZE):\n",
    "        #h_size is number of output filters at the end of the final conv layer\n",
    "        #In a regular DQN, this would just get flattened to get the Q-values for all actions\n",
    "        #But here these output filter maps are going to be sent to the V and A networks\n",
    "        \n",
    "        #We get frame, flatten it using def processState(states) and then reshape it here\n",
    "        \n",
    "        self.scalarInput = tf.placeholder(shape=[None, 84*84*3], dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1, 84, 84, 3])\n",
    "        \n",
    "        #CNN layers\n",
    "        \n",
    "        self.conv1 = slim.conv2d( \n",
    "            inputs=self.imageIn,\n",
    "            num_outputs=32,\n",
    "            kernel_size=[8,8],\n",
    "            stride=[4,4],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        self.conv2 = slim.conv2d( \n",
    "            inputs=self.conv1,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[4,4],\n",
    "            stride=[2,2],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )        \n",
    "        \n",
    "        self.conv3 = slim.conv2d( \n",
    "            inputs=self.conv2,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[3,3],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        self.conv4 = slim.conv2d( \n",
    "            inputs=self.conv3,\n",
    "            num_outputs= H_SIZE,\n",
    "            kernel_size=[7,7],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        # In a regular DQN we'd flatten now and use Fully Connected layers\n",
    "        # But here we split these feature maps in two streams and then flatten them \n",
    "        # Could we have flattened and then split?\n",
    "        \n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3) #Split conv4 output into 2 parts, along last axis\n",
    "        # Last axis here no longer denotes color \n",
    "        \n",
    "        #Flatten\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        \n",
    "        #We put both A and V streams through 1 fully connected layer\n",
    "        #So we need weight matrices for both\n",
    "        #Random normal initialization\n",
    "        self.AW = tf.Variable(tf.random_normal([H_SIZE//2, env.actions])) \n",
    "        self.VW = tf.Variable(tf.random_normal([H_SIZE//2, 1]))\n",
    "        #Advantage values will be 1 per action based on the state\n",
    "        #But just 1 V value for the state\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        #We find net advantage above and over the average advantage\n",
    "        self.netAdv = tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        #Now we add V value and net advantage to get final Q value\n",
    "        self.Qout = self.Value + self.netAdv\n",
    "        #Why + and not tf.add or something??\n",
    "        #Now we predict the Q values for all actions that could be taken from this state\n",
    "        #We could have done this in a loop i.e.1 Q value for the state + one particular action\n",
    "        #But we save time by predicting all the Q values and then taking the action with the largest Q\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Now finally we have made a prediction. Now starts the backward pass\n",
    "        # We need to calc the TD error i.e. MSE between targetQ and Q\n",
    "        #targetQ is coming from the target network at run time\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32) \n",
    "        \n",
    "        \n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32 ) \n",
    "        #In Double DQN the action predicted by the main network isn't whose Q value we use\n",
    "        #self.predict is the action that we have to take\n",
    "        #But it's Q value is actually overestimated, so we give that action to the \n",
    "        #Note that this class will represent both Target and Main networks, so it needs to have variables\n",
    "        #that either one might need.\n",
    "        \n",
    "        #That's why there is self.actions to receive the action integer\n",
    "        #This integer is converted to its corresp Q value by multiplying with the action's one-hot\n",
    "        #representation and then reduce-summing it. All the other Q values will be multiplied by 0 except \n",
    "        #the one corresp to self.actions\n",
    "        #Had it been the usual DQN, we'd have taken self.predict action and used it's corresp self.Q value\n",
    "        #But here we have to send the chosen action (by the main network) to the target network to find out\n",
    "        #what the target network thinks about the Q value, then use that\n",
    "        \n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        #Now the td error\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        #self.Q is a 1D matrix with a particular action's Q value non-zero and rest 0\n",
    "        #self.targetQ has different values coming from target network for the same state\n",
    "        \n",
    "        #Once the loss function has been defined, its all a matter of backprop\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to store exp and then sample to train\n",
    "\n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        #If its going to overflow, then remove those many from the start  \n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer) - self.buffer_size)] = [] # This [] deletes, not adds a [] in its place\n",
    "        self.buffer.extend(experience) #Append would have \"broken\" the smooth array by inserting a sublist\n",
    "        #rather than concatenating the elements of the sublist\n",
    "        \n",
    "    def sample(self, size):\n",
    "        sm = np.array(random.sample(self.buffer, size))\n",
    "        return(np.reshape(sm, [size, 5]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Other functions\n",
    "\n",
    "#To resize game frames\n",
    "def processState(states):\n",
    "    return(np.reshape(states, [21168]))\n",
    "\n",
    "#These two functions allow us to update target network with parameters of primary network\n",
    "\n",
    "def updateTargetGraph(tfVars, TAU):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        addedVal = ( var.value()*TAU ) + ( tfVars[idx+total_vars//2]*(1 - TAU) )\n",
    "        newVal = tfVars[idx+total_vars//2].assign(addedVal)\n",
    "        op_holder.append(newVal)\n",
    "        #The second half of tfVars are the vars belonging to the target network\n",
    "        #1-TAU is like 99.999% , so vast majority of the target network is retained\n",
    "        #Only a small fraction TAU change is allowed based on the main network\n",
    "        return(op_holder)\n",
    "    \n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "#Op holder contains ops, where each op is a weight average update op (using TAU and 1-TAU)\n",
    "#When each sess.run is executed, the target network gets updated one var at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constants and Parameters \n",
    "\n",
    "BATCH_SIZE = 32 #No. of exps per training step\n",
    "UPDATE_FREQ = 4 # Every how many iters of gameplay should training be done(gameplay will continue regardless of training)\n",
    "GAMMA = .99 #Discount factor\n",
    "\n",
    "H_SIZE = 512 #How many filters in final conv layer, multiple of 2 as we are going to split it\n",
    "TAU = 0.001 #How much to update target network gradually\n",
    "\n",
    "START_E = 1 #Start exploration randomness\n",
    "END_E =0.1 #End exploration randomness\n",
    "\n",
    "ANNEALING_STEPS = 10000\n",
    "NUM_EPISODES = 10000\n",
    "\n",
    "PRE_TRAIN_STEPS = 10000 #Warm up to collect experinces before training starts. Purely random actions\n",
    "MAX_EP_LENGTH = 50 #How long each episode\n",
    "\n",
    "LOAD_MODEL = False #Don't use saved model\n",
    "PATH = \"./dqn\" #Where to save model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n",
      "Saved Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-34a46b6dba12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/model-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.cptk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"% of sucessful episodes: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrList\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rahul/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1361\u001b[0m       model_checkpoint_path = sess.run(\n\u001b[1;32m   1362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1364\u001b[0m       \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rahul/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rahul/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rahul/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/rahul/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rahul/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #Reset\n",
    "\n",
    "mainQN = Qnetwork(H_SIZE)\n",
    "targetQN = Qnetwork(H_SIZE) #Idenitical networks\n",
    "\n",
    "init = tf.global_variables_initializer() #Initialize all vars\n",
    "\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "\n",
    "saver = tf.train.Saver() # For saving model\n",
    "trainables = tf.trainable_variables() # Returns all variables created with trainable=True i.e. what all can be trained\n",
    "targetOps=updateTargetGraph(trainables, TAU)\n",
    "\n",
    "#Set exploration rate decrease\n",
    "\n",
    "e = START_E\n",
    "stepDrop = (START_E - END_E)/ANNEALING_STEPS\n",
    "\n",
    "#Hold total reward and reward per ep\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "#Make path\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "    \n",
    "    \n",
    "#Now we start the session in which we will build and train\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #Init op has been run\n",
    "    \n",
    "    #In case we are using a trained model\n",
    "    if LOAD_MODEL == True:\n",
    "        print('Loading model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(PATH) #Checkpoint\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path )\n",
    "        \n",
    "        \n",
    "    updateTarget(targetOps, sess) #Using this session, update the target network with TAU part of main network\n",
    "    \n",
    "    \n",
    "    #Actual running of episode starts here\n",
    "    \n",
    "    for i in range(NUM_EPISODES):\n",
    "        \n",
    "        episodeBuffer = experience_buffer()\n",
    "        \n",
    "        s_raw = env.reset()\n",
    "        s = processState(s_raw)#Reset environment and get first new observation\n",
    "        d = False #Not \"done\" yet\n",
    "        \n",
    "        rAll = 0 #Reset reward for the episode to 0\n",
    "        \n",
    "        j = 0 #Reset number of steps in the episode to 0\n",
    "        \n",
    "        #The actual training of the Q-Networks\n",
    "        while j < MAX_EP_LENGTH:\n",
    "            \n",
    "            j+=1 #Increment steps\n",
    "            \n",
    "            #If warming up or probability < e, then choose a random action. Or get a calculated action\n",
    "            if (np.random.rand(1) < 1) or ( total_steps < PRE_TRAIN_STEPS):\n",
    "                a = np.random.randint(0,4) # action 0,1,2,3\n",
    "                \n",
    "            else:\n",
    "                action_predicted = sess.run( mainQN.predict, feed_dict={\n",
    "                        mainQN.scalarInput : [s]\n",
    "                    } )\n",
    "                # We need the main network to predict an action. Therefore we run it in the current session\n",
    "                # inside which the computational graph has been built.\n",
    "                # In order to run portion of the computational graph that ends with, everything back-connected\n",
    "                # to it will get run. But at the very start, it needs to get data from us before it can process\n",
    "                # it. That data is scalarInput, which we give in the feed_dict\n",
    "                # This is just a forward pass, no training has happening inside this particular execution of\n",
    "                # sess.run. This bit is happening inside the tensorflow machine i.e. likely the GPU\n",
    "                a = action_predicted[0] #We got back a vector, but a has to be an scalar integer\n",
    "                \n",
    "                #Now we know what action has to be taken. Let's take it in the env and see what happens\n",
    "                \n",
    "                s1_raw, r, d = env.step(a) # Take action a, get back new state, reward due to taking action a\n",
    "                # and whther the episode is done or not. In OpenAI Gym interface, an additonal \"info\"\n",
    "                # value is returned but not here\n",
    "                s1 = processState(s1_raw)\n",
    "                \n",
    "                #Let's add this bit of experience into the episode buffer\n",
    "                exp_formatted = np.reshape(np.array([s,a,r,s1,d]) , [1,5]) #Exp list made into an np array\n",
    "                # and then reshaped to 1x5\n",
    "                episodeBuffer.add(exp_formatted)\n",
    "                \n",
    "                # Increment number of steps\n",
    "                total_steps+=1\n",
    "                \n",
    "                \n",
    "                # Now in the warmup phase we don't train. So we first check if warmup is done\n",
    "                # and then start training\n",
    "                \n",
    "                if total_steps > PRE_TRAIN_STEPS:\n",
    "                    #Now we are done with warmup\n",
    "                    \n",
    "                    if e > END_E:\n",
    "                        e-=stepDrop # Reduce e a bit\n",
    "                        \n",
    "                        \n",
    "                    if (total_steps % UPDATE_FREQ) == 0: #Training only every few steps\n",
    "                        \n",
    "                        trainBatch = myBuffer.sample(BATCH_SIZE)\n",
    "                        # We ask for a random training batch of 32 experiences, which probably does not have the exp\n",
    "                        # we pushed in just above in the code. So the agent is being trained on something\n",
    "                        # completely different\n",
    "                        \n",
    "                        # trainBatch is basically s,a,r,s1,d\n",
    "                        # vstack vertically stacks 1x5 experiences, for example\n",
    "                        # At t = 58 s,a,r,s1,d\n",
    "                        # At t = 59 s,a,r,s1,d\n",
    "                        # At t = 60 s,a,r,s1,d\n",
    "                        # At t = 61 s,a,r,s1,d\n",
    "                        # and so on 32 times\n",
    "                        \n",
    "                        # Now trainBatch[:, 3] is a collection of list all 32 s1\n",
    "                        # We need to then stack them vertically like a loaf of bread, just like DQN uses last\n",
    "                        # 4 frames\n",
    "                        \n",
    "                        stacked_next_frames = np.vstack(trainBatch[:, 3])\n",
    "                        \n",
    "                        a_pred = sess.run(mainQN.predict, feed_dict={       \n",
    "                                mainQN.scalarInput : stacked_next_frames    \n",
    "                            })\n",
    "                        \n",
    "                        Q_targ = sess.run(targetQN.Qout, feed_dict={\n",
    "                                targetQN.scalarInput : stacked_next_frames \n",
    "                            }) # 32x4 , has \n",
    "                        \n",
    "                        stacked_done = trainBatch[:, 4] #This an np.array of Falses and perhaps a True \n",
    "                                                        # i.e. episode done\n",
    "                                                        \n",
    "                        end_multiplier =  -(stacked_done - 1) # We can subtract 1 from the np.array of \n",
    "                                                        # Falses->0 and Trues->1 (but not a list though)\n",
    "                                                        # end_multiplier will be = 0 whenever the episode ended\n",
    "                                                        # and +1 elsewhere\n",
    "                                                        # So end_multiplier is an array of mostly 1s and a few \n",
    "                                                        # 0s. This basically nullifies the future Q value\n",
    "                                                        # whenever the episode has ended\n",
    "                                                        \n",
    "                        doubleQ = Q_targ[range(BATCH_SIZE), action_predicted] # This is numpy advanced indexing\n",
    "                                                        # Here we select all 32 rows but only the column corresp\n",
    "                                                        # to action_predicted\n",
    "                                                        \n",
    "                        stacked_reward = trainBatch[ :, 2]\n",
    "                        targetQ = stacked_reward + (GAMMA*doubleQ*end_multiplier)\n",
    "                                                        \n",
    "                        # Now we train the main Q network i.e. backprop\n",
    "                        # We need to give it initial frames and actions \n",
    "                        # After all our network is approximating the Q value\n",
    "                        # Hence it needs to take state and actions as inputs\n",
    "                                                        \n",
    "                        stacked_initial_frames = np.vstack(trainBatch[ : , 0])\n",
    "                        stacked_actions = trainBatch[:, 1]\n",
    "                                                        \n",
    "                        _ = sess.run(mainQN.updateModel, feed_dict={\n",
    "                                    mainQN.scalarInput : stacked_initial_frames,\n",
    "                                    mainQN.actions : stacked_actions,\n",
    "                                    mainQN.targetQ : targetQ\n",
    "                                })\n",
    "                        # To run updateModel of mainQN inside the QNetwork class we need to run loss\n",
    "                        # which in turn depends on td_error which depends on targetQ and Q\n",
    "                        # TargetQ will come from the target network\n",
    "                        # It's own Q comes Q_out and the action selected by the main network earlier in the game???\n",
    "                        # These actions are saved in the buffer. And of course the initial state frames\n",
    "                                                        \n",
    "                        updateTarget(targetOps, sess) # Slowly bring Target network closer to main network\n",
    "                                                        \n",
    "                #Now we are at the end of a step                                    \n",
    "                rAll+=r\n",
    "                s=s1\n",
    "                if d == True:break #If step led to episode over, then break\n",
    "                \n",
    "            #Now we are at the end of episode\n",
    "            myBuffer.add(episodeBuffer.buffer) #Save the whole episode in another buffer\n",
    "            jList.append(j) #How many steps we had in the episode that just ended\n",
    "            rList.append(rAll) #How much reward we got over all the steps in this just ended episode\n",
    "                                                        \n",
    "            #Save model often\n",
    "            if i % 1000 == 0:\n",
    "                    saver.save(sess, PATH+'/model-'+str(i)+'.cptk')\n",
    "                    print(\"Saved Model\")\n",
    "                                                        \n",
    "            #\n",
    "        saver.save(sess,PATH+'/model-'+str(i)+'.cptk')\n",
    "print(\"% of sucessful episodes: \" + str(sum(rList/NUM_EPISODES)) + \"%\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Variable            Type                          Data/Info\n",
    "-----------------------------------------------------------\n",
    "Q1                  ndarray                       32: 32 elems, type `int64`, 256 bytes\n",
    "Q2                  ndarray                       32x4: 128 elems, type `float32`, 512 bytes\n",
    "Qnetwork            classobj                      __main__.Qnetwork\n",
    "a                   int                           2\n",
    "anneling_steps      float                         10000.0\n",
    "batch_size          int                           32\n",
    "d                   bool                          False\n",
    "division            __future__._Feature           _Feature((2, 2, 0, 'alpha<...> 0, 0, 'alpha', 0), 8192)\n",
    "doubleQ             ndarray                       32: 32 elems, type `float32`, 128 bytes\n",
    "e                   float                         0.91252\n",
    "endE                float                         0.1\n",
    "end_multiplier      ndarray                       32: 32 elems, type `object`, 256 bytes\n",
    "env                 gridworld.gameEnv             <gridworld.gameEnv instance at 0x7f14d068bcb0>\n",
    "episodeBuffer       __main__.experience_buffer    <__main__.experience_buff<...>stance at 0x7f147489be18>\n",
    "experience_buffer   classobj                      __main__.experience_buffer\n",
    "gameEnv             classobj                      gridworld.gameEnv\n",
    "gym                 module                        <module 'gym' from '/home<...>ckages/gym/__init__.pyc'>\n",
    "h_size              int                           512\n",
    "i                   int                           219\n",
    "init                Operation                     name: \"init\"\\nop: \"NoOp\"\\<...>riable_3/Adam_1/Assign\"\\n\n",
    "j                   int                           22\n",
    "jList               list                          n=219\n",
    "load_model          bool                          False\n",
    "mainQN              __main__.Qnetwork             <__main__.Qnetwork instance at 0x7f14750ffbd8>\n",
    "max_epLength        int                           50\n",
    "myBuffer            __main__.experience_buffer    <__main__.experience_buff<...>stance at 0x7f1474891e60>\n",
    "np                  module                        <module 'numpy' from '/ho<...>ages/numpy/__init__.pyc'>\n",
    "num_episodes        int                           10000\n",
    "os                  module                        <module 'os' from '/home/<...>ow/lib/python2.7/os.pyc'>\n",
    "path                str                           ./dqn\n",
    "plt                 module                        <module 'matplotlib.pyplo<...>s/matplotlib/pyplot.pyc'>\n",
    "pre_train_steps     int                           10000\n",
    "processState        function                      <function processState at 0x7f14751aa5f0>\n",
    "r                   float                         0.0\n",
    "rAll                float                         0.0\n",
    "rList               list                          n=219\n",
    "random              module                        <module 'random' from '/h<...>ib/python2.7/random.pyc'>\n",
    "s                   ndarray                       21168: 21168 elems, type `uint8`, 21168 bytes\n",
    "s1                  ndarray                       21168: 21168 elems, type `uint8`, 21168 bytes\n",
    "saver               Saver                         <tensorflow.python.traini<...>object at 0x7f1474bbdf10>\n",
    "scipy               module                        <module 'scipy' from '/ho<...>ages/scipy/__init__.pyc'>\n",
    "sess                Session                       <tensorflow.python.client<...>object at 0x7f1475005f90>\n",
    "slim                module                        <module 'tensorflow.contr<...>ntrib/slim/__init__.pyc'>\n",
    "startE              int                           1\n",
    "stepDrop            float                         9e-05\n",
    "targetOps           list                          n=6\n",
    "targetQ             ndarray                       32: 32 elems, type `object`, 256 bytes\n",
    "targetQN            __main__.Qnetwork             <__main__.Qnetwork instance at 0x7f1474c1c4d0>\n",
    "tau                 float                         0.001\n",
    "tf                  module                        <module 'tensorflow' from<...>tensorflow/__init__.pyc'>\n",
    "total_steps         int                           10972\n",
    "trainBatch          ndarray                       32x5: 160 elems, type `object`, 1280 bytes\n",
    "trainables          list                          n=12\n",
    "updateTarget        function                      <function updateTarget at 0x7f14750250c8>\n",
    "updateTargetGraph   function                      <function updateTargetGraph at 0x7f147503d500>\n",
    "update_freq         int                           4\n",
    "y                   float                         0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
