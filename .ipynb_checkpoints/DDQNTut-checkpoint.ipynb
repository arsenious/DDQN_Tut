{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(tf.add(40,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAC95JREFUeJzt3V2MXdV9hvHnrYGQQBtwTBHF0PEFIrIiYVKLQomqFHDk\n0Ij0CoFEFVVI3KQtVJGi0F6g3HFRRclFFQlBUtRQUupAg6yIlCREVaXKwXw0BX/EhJhgF2KTNiWl\nUlon/16cbTGxPPYez5kzs7yenzSas9c51tnL9jt7z549601VIak/v7LSOyBpZRh+qVOGX+qU4Zc6\nZfilThl+qVOGX+rUksKfZGuSvUleSvKpae2UpOWXU73JJ8ka4HvAFuAA8DRwa1Xtmt7uSVouZyzh\nz14FvFRVLwMk+TLwUWDB8K9bt67m5uaW8JaSTmT//v288cYbGfPapYT/YuDVedsHgN8+0R+Ym5tj\n586dS3hLSSeyefPm0a9d9gt+Se5IsjPJzsOHDy/320kaaSnhPwhcMm97/TD2S6rqvqraXFWbL7jg\ngiW8naRpWkr4nwYuS7IhyVnALcDj09ktScvtlL/nr6ojSf4Y+DqwBvhCVb04tT2TtKyWcsGPqvoa\n8LUp7YukGfIOP6lThl/qlOGXOmX4pU4ZfqlThl/qlOGXOmX4pU4ZfqlThl/qlOGXOmX4pU4ZfqlT\nhl/qlOGXOmX4pU4ZfqlTJw1/ki8kOZTkhXlja5M8mWTf8Pn85d1NSdM25sj/18DWY8Y+BXyzqi4D\nvjlsS2rIScNfVf8E/Mcxwx8FHhwePwj8wZT3S9IyO9Xv+S+sqteGx68DF05pfyTNyJIv+NWk6XPB\ntk8be6TV6VTD/6MkFwEMnw8t9EIbe6TV6VTD/zjwseHxx4CvTmd3JM3KSUs7kjwMfBBYl+QAcA9w\nL/BIktuBV4Cbl3MnpyEZ1Vp8+lnwGzLNQq3if4CThr+qbl3gqeunvC+SZsg7/KROGX6pU4Zf6pTh\nlzpl+KVOGX6pU4Zf6pThlzpl+KVOGX6pU4Zf6pThlzpl+KVOGX6pU4Zf6pThlzpl+KVOjWnsuSTJ\nU0l2JXkxyZ3DuK09UsPGHPmPAJ+oqo3A1cDHk2zE1h6paWMae16rqmeHxz8FdgMXY2uP1LRFfc+f\nZA64EtjByNYeSzuk1Wl0+JOcC3wFuKuq3pz/3IlaeyztkFanUeFPciaT4D9UVY8Ow6NbeyStPmOu\n9gd4ANhdVZ+Z95StPVLDTlraAVwL/CHwb0meH8b+nAZbeyS9bUxjzz8DC3Vd2dojNco7/KROGX6p\nU4Zf6pThlzpl+KVOGX6pU2N+zq+G1UI/pJ2RHP+u7xlbwb+E1TD9BXjklzpl+KVOGX6pU4Zf6pTh\nlzpl+KVOGX6pU4Zf6pThlzpl+KVOjVnD7+wk30nyr0Njz6eHcRt7pIaNOfL/DLiuqq4ANgFbk1yN\njT1S08Y09lRV/feweebwUdjYIzVt7Lr9a4aVew8BT1aVjT1S40aFv6p+XlWbgPXAVUned8zzNvZI\njVnU1f6q+gnwFLAVG3ukpo252n9BkvOGx+8EtgB7sLFHatqYlXwuAh5MsobJF4tHqmp7kn/Bxh6p\nWWMae77LpJb72PEfY2OP1Czv8JM6ZfilThl+qVOGX+qU4Zc6ZfilThl+qVOGX+qUXX2nuRWu6lsV\ne6Dj88gvdcrwS50y/FKnDL/UKcMvdcrwS50y/FKnDL/UqdHhH5bvfi7J9mHbxh6pYYs58t8J7J63\nbWOP1LCxpR3rgd8H7p83bGOP1LCxR/7PAp8EfjFvzMYeqWFj1u3/CHCoqp5Z6DU29kjtGfNbfdcC\nNyW5ETgb+LUkX2Jo7Kmq12zskdozpqX37qpaX1VzwC3At6rqNmzskZq2lJ/z3wtsSbIPuGHYltSI\nRS3mUVXfBr49PLaxR2qYd/hJnTL8UqcMv9Qpwy91yvBLnTL8UqcMv9Qpwy91yvBLnTL8UqcMv9Qp\nwy91yvBLnTL8UqcW9Su9as9x11aboazw+2thHvmlTo068ifZD/wU+DlwpKo2J1kL/B0wB+wHbq6q\n/1ye3ZQ0bYs58v9eVW2qqs3DtqUdUsOWctpvaYfUsLHhL+AbSZ5JcscwNqq0Q9LqNPZq/weq6mCS\nXweeTLJn/pNVVUmOe2F5+GJxB8Cll166pJ2VND2jjvxVdXD4fAh4DLiKobQD4ESlHTb2SKvTmLqu\nc5L86tHHwIeAF7C0Q2ramNP+C4HHkhx9/d9W1RNJngYeSXI78Apw8/LtpqRpO2n4q+pl4IrjjFva\nITXMO/ykThl+qVOGX+qU4Zc6ZfilThl+qVOGX+qU4Zc6ZfilThl+qVOGX+qU4Zc6ZfilThl+qVOG\nX+qU4Zc6ZfilTo0Kf5LzkmxLsifJ7iTXJFmb5Mkk+4bP5y/3zkqanrFH/s8BT1TVe5ks6bUbG3uk\npo1ZvffdwO8CDwBU1f9W1U+wsUdq2pgj/wbgMPDFJM8luX9YwtvGHqlhY8J/BvB+4PNVdSXwFsec\n4ldVsUAVfJI7kuxMsvPw4cNL3V9JUzIm/AeAA1W1Y9jexuSLgY09DUit7IdWr5OGv6peB15Ncvkw\ndD2wCxt7pKaNLer8E+ChJGcBLwN/xOQLh409UqNGhb+qngc2H+cpG3ukRnmHn9Qpwy91yvBLnTL8\nUqcMv9Qpwy91yvBLnTL8UqcMv9Qpwy91yvBLnTL8UqcMv9Qpwy91yvBLnTL8UqcMv9SpMev2X57k\n+Xkfbya5y8YeqW1jFvDcW1WbqmoT8FvA/wCPYWOP1LTFnvZfD3y/ql7Bxh6paYsN/y3Aw8NjG3uk\nho0O/7Bs903A3x/7nI09UnsWc+T/MPBsVf1o2LaxR2rYYsJ/K2+f8oONPVLTRoV/aOXdAjw6b/he\nYEuSfcANw7akRoxt7HkLeM8xYz/Gxh6pWd7hJ3VqbFFn8yY/kNDM+de+annklzpl+KVOGX6pU4Zf\n6pThlzpl+KVOGX6pU4Zf6pThlzpl+KVOGX6pU4Zf6pThlzpl+KVOGX6pU2OX8fqzJC8meSHJw0nO\ntrFHatuYuq6LgT8FNlfV+4A1TNbvt7FHatjY0/4zgHcmOQN4F/Dv2NgjNW1MV99B4C+BHwKvAf9V\nVf+IjT1S08ac9p/P5Ci/AfgN4Jwkt81/jY09UnvGnPbfAPygqg5X1f8xWbv/d7CxR2ramPD/ELg6\nybuShMla/buxsUdq2kmX7q6qHUm2Ac8CR4DngPuAc4FHktwOvALcvJw7Kmm6xjb23APcc8zwz7Cx\nR2qWd/hJnTL8UqcMv9Qpwy91KrMssExyGHgLeGNmb7r81uF8VrPTaT5j5vKbVTXqhpqZhh8gyc6q\n2jzTN11Gzmd1O53mM+25eNovdcrwS51aifDftwLvuZycz+p2Os1nqnOZ+ff8klYHT/ulTs00/Em2\nJtmb5KUkTS37leSSJE8l2TWsZ3jnMN70WoZJ1iR5Lsn2YbvZ+SQ5L8m2JHuS7E5yTePzWda1M2cW\n/iRrgL8CPgxsBG5NsnFW7z8FR4BPVNVG4Grg48P+t76W4Z1MfkX7qJbn8zngiap6L3AFk3k1OZ+Z\nrJ1ZVTP5AK4Bvj5v+27g7lm9/zLM56vAFmAvcNEwdhGwd6X3bRFzWD/8B7oO2D6MNTkf4N3ADxiu\nY80bb3U+FwOvAmuZ/PbtduBD05zPLE/7j07mqAPDWHOSzAFXAjtoey3DzwKfBH4xb6zV+WwADgNf\nHL6NuT/JOTQ6n5rB2ple8FukJOcCXwHuqqo35z9Xky/HTfz4JMlHgENV9cxCr2lpPkyOju8HPl9V\nVzK5jfyXTolbms9S184cY5bhPwhcMm97/TDWjCRnMgn+Q1X16DA8ai3DVeha4KYk+4EvA9cl+RLt\nzucAcKCqdgzb25h8MWh1PktaO3OMWYb/aeCyJBuSnMXk4sXjM3z/JRnWL3wA2F1Vn5n3VJNrGVbV\n3VW1vqrmmPxbfKuqbqPd+bwOvJrk8mHoemAXjc6HWaydOeOLGDcC3wO+D/zFSl9UWeS+f4DJKdZ3\ngeeHjxuB9zC5aLYP+AawdqX39RTm9kHevuDX7HyATcDO4d/oH4DzG5/Pp4E9wAvA3wDvmOZ8vMNP\n6pQX/KROGX6pU4Zf6pThlzpl+KVOGX6pU4Zf6pThlzr1/1FPNEuV2wvbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efdf964bd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load game env\n",
    "\n",
    "from gridworld import gameEnv\n",
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, H_SIZE):\n",
    "        #h_size is number of output filters at the end of the final conv layer\n",
    "        #In a regular DQN, this would just get flattened to get the Q-values for all actions\n",
    "        #But here these output filter maps are going to be sent to the V and A networks\n",
    "        \n",
    "        #We get frame, flatten it using def processState(states) and then reshape it here\n",
    "        \n",
    "        self.scalarInput = tf.placeholder(shape=[None, 84*84*3], dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1, 84, 84, 3])\n",
    "        \n",
    "        #CNN layers\n",
    "        \n",
    "        self.conv1 = slim.conv2d( \n",
    "            inputs=self.imageIn,\n",
    "            num_outputs=32,\n",
    "            kernel_size=[8,8],\n",
    "            stride=[4,4],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        self.conv2 = slim.conv2d( \n",
    "            inputs=self.conv1,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[4,4],\n",
    "            stride=[2,2],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )        \n",
    "        \n",
    "        self.conv3 = slim.conv2d( \n",
    "            inputs=self.conv2,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[3,3],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        self.conv4 = slim.conv2d( \n",
    "            inputs=self.conv3,\n",
    "            num_outputs=h_size,\n",
    "            kernel_size=[7,7],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        # In a regular DQN we'd flatten now and use Fully Connected layers\n",
    "        # But here we split these feature maps in two streams and then flatten them \n",
    "        # Could we have flattened and then split?\n",
    "        \n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3) #Split conv4 output into 2 parts, along last axis\n",
    "        # Last axis here no longer denotes color \n",
    "        \n",
    "        #Flatten\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        \n",
    "        #We put both A and V streams through 1 fully connected layer\n",
    "        #So we need weight matrices for both\n",
    "        #Random normal initialization\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2, env.actions])) \n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        #Advantage values will be 1 per action based on the state\n",
    "        #But just 1 V value for the state\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        #We find net advantage above and over the average advantage\n",
    "        self.netAdv = tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        #Now we add V value and net advantage to get final Q value\n",
    "        self.Qout = self.Value + self.netAdv\n",
    "        #Why + and not tf.add or something??\n",
    "        #Now we predict the Q values for all actions that could be taken from this state\n",
    "        #We could have done this in a loop i.e.1 Q value for the state + one particular action\n",
    "        #But we save time by predicting all the Q values and then taking the action with the largest Q\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Now finally we have made a prediction. Now starts the backward pass\n",
    "        # We need to calc the TD error i.e. MSE between targetQ and Q\n",
    "        #targetQ is coming from the target network at run time\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32) \n",
    "        \n",
    "        \n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32 ) \n",
    "        #In Double DQN the action predicted by the main network isn't whose Q value we use\n",
    "        #self.predict is the action that we have to take\n",
    "        #But it's Q value is actually overestimated, so we give that action to the \n",
    "        #Note that this class will represent both Target and Main networks, so it needs to have variables\n",
    "        #that either one might need.\n",
    "        \n",
    "        #That's why there is self.actions to receive the action integer\n",
    "        #This integer is converted to its corresp Q value by multiplying with the action's one-hot\n",
    "        #representation and then reduce-summing it. All the other Q values will be multiplied by 0 except \n",
    "        #the one corresp to self.actions\n",
    "        #Had it been the usual DQN, we'd have taken self.predict action and used it's corresp self.Q value\n",
    "        #But here we have to send the chosen action (by the main network) to the target network to find out\n",
    "        #what the target network thinks about the Q value, then use that\n",
    "        \n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype-tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        #Now the td error\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        #self.Q is a 1D matrix with a particular action's Q value non-zero and rest 0\n",
    "        #self.targetQ has different values coming from target network for the same state\n",
    "        \n",
    "        #Once the loss function has been defined, its all a matter of backprop\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to store exp and then sample to train\n",
    "\n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        #If its going to overflow, then remove those many from the start  \n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer) - self.buffer_size)] = [] # This [] deletes, not adds a [] in its place\n",
    "        self.buffer.extend(experience) #Append would have \"broken\" the smooth array by inserting a sublist\n",
    "        #rather than concatenating the elements of the sublist\n",
    "        \n",
    "    def sample(self, size):\n",
    "        sm = np.array(random.sample(self.buffer, size))\n",
    "        return(np.reshape(sm, [size, 5]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Other functions\n",
    "\n",
    "#To resize game frames\n",
    "def processState(states):\n",
    "    return(np.reshape(states, [21168]))\n",
    "\n",
    "#These two functions allow us to update target network with parameters of primary network\n",
    "\n",
    "def updateTargetGraph(tfVars, TAU):\n",
    "    total_vars = len(tfVars)\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        addedVal = ( var.value()*TAU ) + ( tfVars[idx+total_vars//2]*(1 - TAU) )\n",
    "        newVal = tfVars[idx+total_vars//2].assign(addedVal)\n",
    "        op_holder.append(newVal)\n",
    "        #The second half of tfVars are the vars belonging to the target network\n",
    "        #1-TAU is like 99.999% , so vast majority of the target network is retained\n",
    "        #Only a small fraction TAU change is allowed based on the main network\n",
    "        return(op_holder)\n",
    "    \n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "#Op holder contains ops, where each op is a weight average update op (using TAU and 1-TAU)\n",
    "#When each sess.run is executed, the target network gets updated one var at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constants and Parameters \n",
    "\n",
    "BATCH_SIZE = 32 #No. of exps per training step\n",
    "UPDATE_FREQ = 4 # Every how many iters of gameplay should training be done(gameplay will continue regardless of training)\n",
    "GAMMA = .99 #Discount factor\n",
    "\n",
    "H_SIZE = 512 #How many filters in final conv layer, multiple of 2 as we are going to split it\n",
    "TAU = 0.001 #How much to update target network gradually\n",
    "\n",
    "START_E = 1 #Start exploration randomness\n",
    "END_E =0.1 #End exploration randomness\n",
    "\n",
    "ANNEALING_STEPS = 10000\n",
    "NUM_EPISODES = 10000\n",
    "\n",
    "PRE_TRAIN_STEPS = 10000 #Warm up to collect experinces before training starts. Purely random actions\n",
    "MAX_EP_LENGTH = 50 #How long each episode\n",
    "\n",
    "LOAD_MODEL = False #Don't use saved model\n",
    "PATH = \"./dqn\" #Where to save model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-9-4408b5fe3203>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-4408b5fe3203>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #Reset\n",
    "\n",
    "init = tf.global_variables_initializer() #Initialize all vars\n",
    "saver = tf.train.Saver() # For saving model\n",
    "trainables = tf.trainable_variables() # Returns all variables created with trainable=True i.e. what all can be trained\n",
    "\n",
    "mainQN = Qnetwork(H_SIZE)\n",
    "targetQN = Qnetwork(H_SIZE) #Idenitical networks\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set exploration rate decrease\n",
    "\n",
    "e = START_E\n",
    "stepDrop = (START_E - END_E)/ANNEALING_STEPS\n",
    "\n",
    "#Hold total reward and reward per ep\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "#Make path\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "    \n",
    "    \n",
    "#Now we start the session in which we will build and train\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #Init op has been run\n",
    "    \n",
    "    #In case we are using a trained model\n",
    "    if LOAD_MODEL == True:\n",
    "        print('Loading model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(PATH) #Checkpoint\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path )\n",
    "        \n",
    "        \n",
    "    updateTarget(targetOps, sess) #Using this session, update the target network with TAU part of main network\n",
    "    \n",
    "    \n",
    "    #Actual running of episode starts here\n",
    "    \n",
    "    for i in range(NUM_EPISODES):\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
