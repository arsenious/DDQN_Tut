{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(tf.add(40,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81444d69d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load game env\n",
    "from gridworld import gameEnv\n",
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, H_SIZE):\n",
    "        #h_size is number of output filters at the end of the final conv layer\n",
    "        #In a regular DQN, this would just get flattened to get the Q-values for all actions\n",
    "        #But here these output filter maps are going to be sent to the V and A networks\n",
    "        \n",
    "        #We get frame, flatten it using def processState(states) and then reshape it here\n",
    "        \n",
    "        self.scalarInput = tf.placeholder(shape=[None, 84*84*3], dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1, 84, 84, 3])\n",
    "        \n",
    "        #CNN layers\n",
    "        \n",
    "        self.conv1 = slim.conv2d( \n",
    "            inputs=self.imageIn,\n",
    "            num_outputs=32,\n",
    "            kernel_size=[8,8],\n",
    "            stride=[4,4],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        self.conv2 = slim.conv2d( \n",
    "            inputs=self.conv1,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[4,4],\n",
    "            stride=[2,2],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )        \n",
    "        \n",
    "        self.conv3 = slim.conv2d( \n",
    "            inputs=self.conv2,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[3,3],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        self.conv4 = slim.conv2d( \n",
    "            inputs=self.conv3,\n",
    "            num_outputs=h_size,\n",
    "            kernel_size=[7,7],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        # In a regular DQN we'd flatten now and use Fully Connected layers\n",
    "        # But here we split these feature maps in two streams and then flatten them \n",
    "        # Could we have flattened and then split?\n",
    "        \n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3) #Split conv4 output into 2 parts, along last axis\n",
    "        # Last axis here no longer denotes color \n",
    "        \n",
    "        #Flatten\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        \n",
    "        #We put both A and V streams through 1 fully connected layer\n",
    "        #So we need weight matrices for both\n",
    "        #Random normal initialization\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2, env.actions])) \n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        #Advantage values will be 1 per action based on the state\n",
    "        #But just 1 V value for the state\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        #We find net advantage above and over the average advantage\n",
    "        self.netAdv = tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        #Now we add V value and net advantage to get final Q value\n",
    "        self.Qout = self.Value + self.netAdv\n",
    "        #Why + and not tf.add or something??\n",
    "        #Now we predict the Q values for all actions that could be taken from this state\n",
    "        #We could have done this in a loop i.e.1 Q value for the state + one particular action\n",
    "        #But we save time by predicting all the Q values and then taking the action with the largest Q\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Now finally we have made a prediction. Now starts the backward pass\n",
    "        # We need to calc the TD error i.e. MSE between targetQ and Q\n",
    "        #targetQ is coming from the target network at run time\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32) \n",
    "        \n",
    "        \n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32 ) \n",
    "        #In Double DQN the action predicted by the main network isn't whose Q value we use\n",
    "        #self.predict is the action that we have to take\n",
    "        #But it's Q value is actually overestimated, so we give that action to the \n",
    "        #Note that this class will represent both Target and Main networks, so it needs to have variables\n",
    "        #that either one might need.\n",
    "        \n",
    "        #That's why there is self.actions to receive the action integer\n",
    "        #This integer is converted to its corresp Q value by multiplying with the action's one-hot\n",
    "        #representation and then reduce-summing it. All the other Q values will be multiplied by 0 except \n",
    "        #the one corresp to self.actions\n",
    "        #Had it been the usual DQN, we'd have taken self.predict action and used it's corresp self.Q value\n",
    "        #But here we have to send the chosen action (by the main network) to the target network to find out\n",
    "        #what the target network thinks about the Q value, then use that\n",
    "        \n",
    "        self.actions_onehot = tf.one_hot(self.actions, env.actions, dtype-tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        #Now the td error\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        #self.Q is a 1D matrix with a particular action's Q value non-zero and rest 0\n",
    "        #self.targetQ has different values coming from target network for the same state\n",
    "        \n",
    "        #Once the loss function has been defined, its all a matter of backprop\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class to store exp and then sample to train\n",
    "\n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        #If its going to overflow, then remove those many from the start  \n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience) + len(self.buffer) - self.buffer_size)] = [] # This [] deletes, not adds a [] in its place\n",
    "        self.buffer.extend(experience) #Append would have \"broken\" the smooth array by inserting a sublist\n",
    "        #rather than concatenating the elements of the sublist\n",
    "        \n",
    "    def sample(self, size):\n",
    "        sm = np.array(random.sample(self.buffer, size))\n",
    "        return(np.reshape(sm, [size, 5]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Other functions\n",
    "\n",
    "#To resize game frames\n",
    "def processState(states):\n",
    "    return(np.reshape(states, [21168]))\n",
    "\n",
    "#These two functions allow us to update target network with parameters of primary network\n",
    "\n",
    "def updateTargetGraph(tfVars, TAU):\n",
    "    total_vars = len(tfVars)\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        addedVal = ( var.value()*TAU ) + ( tfVars[idx+total_vars//2]*(1 - TAU) )\n",
    "        newVal = tfVars[idx+total_vars//2].assign(addedVal)\n",
    "        op_holder.append(newVal)\n",
    "        #The second half of tfVars are the vars belonging to the target network\n",
    "        #1-TAU is like 99.999% , so vast majority of the target network is retained\n",
    "        #Only a small fraction TAU change is allowed based on the main network\n",
    "        return(op_holder)\n",
    "    \n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "#Op holder contains ops, where each op is a weight average update op (using TAU and 1-TAU)\n",
    "#When each sess.run is executed, the target network gets updated one var at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constants and Parameters \n",
    "\n",
    "BATCH_SIZE = 32 #No. of exps per training step\n",
    "UPDATE_FREQ = 4 # Every how many iters of gameplay should training be done(gameplay will continue regardless of training)\n",
    "GAMMA = .99 #Discount factor\n",
    "\n",
    "H_SIZE = 512 #How many filters in final conv layer, multiple of 2 as we are going to split it\n",
    "TAU = 0.001 #How much to update target network gradually\n",
    "\n",
    "START_E = 1 #Start exploration randomness\n",
    "END_E =0.1 #End exploration randomness\n",
    "\n",
    "ANNEALING_STEPS = 10000\n",
    "NUM_EPISODES = 10000\n",
    "\n",
    "PRE_TRAIN_STEPS = 10000 #Warm up to collect experinces before training starts. Purely random actions\n",
    "MAX_EP_LENGTH = 50 #How long each episode\n",
    "\n",
    "LOAD_MODEL = False #Don't use saved model\n",
    "PATH = \"./dqn\" #Where to save model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2d5ce5216296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Initialize all vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# For saving model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrainables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Returns all variables created with trainable=True i.e. what all can be trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rahul/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_step_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_step_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rahul/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to save\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m       self.saver_def = self._builder.build(\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #Reset\n",
    "\n",
    "init = tf.global_variables_initializer() #Initialize all vars\n",
    "saver = tf.train.Saver() # For saving model\n",
    "trainables = tf.trainable_variables() # Returns all variables created with trainable=True i.e. what all can be trained\n",
    "\n",
    "mainQN = Qnetwork(H_SIZE)\n",
    "targetQN = Qnetwork(H_SIZE) #Idenitical networks\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set exploration rate decrease\n",
    "\n",
    "e = START_E\n",
    "stepDrop = (START_E - END_E)/ANNEALING_STEPS\n",
    "\n",
    "#Hold total reward and reward per ep\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "\n",
    "#Make path\n",
    "if not os.path.exists(PATH):\n",
    "    os.makedirs(PATH)\n",
    "    \n",
    "    \n",
    "#Now we start the session in which we will build and train\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) #Init op has been run\n",
    "    \n",
    "    #In case we are using a trained model\n",
    "    if LOAD_MODEL == True:\n",
    "        print('Loading model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(PATH) #Checkpoint\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path )\n",
    "        \n",
    "        \n",
    "    updateTarget(targetOps, sess) #Using this session, update the target network with TAU part of main network\n",
    "    \n",
    "    \n",
    "    #Actual running of episode starts here\n",
    "    \n",
    "    for i in range(NUM_EPISODES):\n",
    "        \n",
    "        episodeBuffer = experience_buffer()\n",
    "        \n",
    "        s = env.reset(processState(s)) #Reset environment and get first new observation\n",
    "        d = False #Not \"done\" yet\n",
    "        \n",
    "        rAll = 0 #Reset reward for the episode to 0\n",
    "        \n",
    "        j = 0 #Reset number of steps in the episode to 0\n",
    "        \n",
    "        #The actual training of the Q-Networks\n",
    "        while j < MAX_EP_LENGTH:\n",
    "            \n",
    "            j+=1 #Increment steps\n",
    "            \n",
    "            #If warming up or probability < e, then choose a random action. Or get a calculated action\n",
    "            if (np.random.rand(1) < 1) or ( total_steps < PRE_TRAIN_STEPS):\n",
    "                a = np.random.randint(0,4) # action 0,1,2,3\n",
    "                \n",
    "            else:\n",
    "                action_predicted = sess.run( mainQN.predict, feed_dict={\n",
    "                        mainQN.scalarInput : [s]\n",
    "                    } )\n",
    "                # We need the main network to predict an action. Therefore we run it in the current session\n",
    "                # inside which the computational graph has been built.\n",
    "                # In order to run portion of the computational graph that ends with, everything back-connected\n",
    "                # to it will get run. But at the very start, it needs to get data from us before it can process\n",
    "                # it. That data is scalarInput, which we give in the feed_dict\n",
    "                # This is just a forward pass, no training has happening inside this particular execution of\n",
    "                # sess.run. This bit is happening inside the tensorflow machine i.e. likely the GPU\n",
    "                a = action_predicted[0] #We got back a vector, but a has to be an scalar integer\n",
    "                \n",
    "                #Now we know what action has to be taken. Let's take it in the env and see what happens\n",
    "                \n",
    "                s1_raw, r, d = env.step(a) # Take action a, get back new state, reward due to taking action a\n",
    "                # and whther the episode is done or not. In OpenAI Gym interface, an additonal \"info\"\n",
    "                # value is returned but not here\n",
    "                s1 = processState(s1_raw)\n",
    "                \n",
    "                #Let's add this bit of experience into the episode buffer\n",
    "                exp_formatted = np.reshape(np.array([s,a,r,s1,d]) , [1,5]) #Exp list made into an np array\n",
    "                # and then reshaped to 1x5\n",
    "                episodeBuffer.add(exp_formatted)\n",
    "                \n",
    "                # Increment number of steps\n",
    "                total_steps+=1\n",
    "                \n",
    "                \n",
    "                # Now in the warmup phase we don't train. So we first check if warmup is done\n",
    "                # and then start training\n",
    "                \n",
    "                if total_steps > PRE_TRAIN_STEPS:\n",
    "                    #Now we are done with warmup\n",
    "                    \n",
    "                    if e > END_E:\n",
    "                        e-=stepDrop # Reduce e a bit\n",
    "                        \n",
    "                        \n",
    "                    if (total_steps % UPDATE_FREQ) == 0: #Training only every few steps\n",
    "                        \n",
    "                        trainBatch = myBuffer.sample(BATCH_SIZE)\n",
    "                        # We ask for a random training batch of 32 experiences, which probably does not have the exp\n",
    "                        # we pushed in just above in the code. So the agent is being trained on something\n",
    "                        # completely different\n",
    "                        \n",
    "                        # trainBatch is basically s,a,r,s1,d\n",
    "                        # vstack vertically stacks 1x5 experiences, for example\n",
    "                        # At t = 58 s,a,r,s1,d\n",
    "                        # At t = 59 s,a,r,s1,d\n",
    "                        # At t = 60 s,a,r,s1,d\n",
    "                        # At t = 61 s,a,r,s1,d\n",
    "                        # and so on 32 times\n",
    "                        \n",
    "                        # Now trainBatch[:, 3] is a collection of list all 32 s1\n",
    "                        # We need to then stack them vertically like a loaf of bread, just like DQN uses last\n",
    "                        # 4 frames\n",
    "                        \n",
    "                        stacked_next_frames = np.vstack(trainBatch[:, 3]\n",
    "                        \n",
    "                        a_pred = sess.run(mainQN.predict, feed_dict={\n",
    "                                mainQN.scalarInput : stacked_next_frames \n",
    "                            })\n",
    "                        \n",
    "                        Q_targ = sess.run(targetQN.Qout, feed_dict={\n",
    "                                targetQN.scalarInput : stacked_next_frames \n",
    "                            }) # 32x4 , has \n",
    "                        \n",
    "                        stacked_done = trainBatch[:, 4] #This an np.array of Falses and perhaps a True \n",
    "                                                        # i.e. episode done\n",
    "                                                        \n",
    "                        end_multiplier =  -(stacked_done - 1) # We can subtract 1 from the np.array of \n",
    "                                                        # Falses->0 and Trues->1 (but not a list though)\n",
    "                                                        # end_multiplier will be = 0 whenever the episode ended\n",
    "                                                        # and +1 elsewhere\n",
    "                                                        # So end_multiplier is an array of mostly 1s and a few \n",
    "                                                        # 0s. This basically nullifies the future Q value\n",
    "                                                        # whenever the episode has ended\n",
    "                                                        \n",
    "                        doubleQ = Q_targ[range(BATCH_SIZE), action_predicted] # This is numpy advanced indexing\n",
    "                                                        # Here we select all 32 rows but only the column corresp\n",
    "                                                        # to action_predicted\n",
    "                                                        \n",
    "                                                        \n",
    "                                                        \n",
    "                                                        \n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0, -1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Variable            Type                          Data/Info\n",
    "-----------------------------------------------------------\n",
    "Q1                  ndarray                       32: 32 elems, type `int64`, 256 bytes\n",
    "Q2                  ndarray                       32x4: 128 elems, type `float32`, 512 bytes\n",
    "Qnetwork            classobj                      __main__.Qnetwork\n",
    "a                   int                           2\n",
    "anneling_steps      float                         10000.0\n",
    "batch_size          int                           32\n",
    "d                   bool                          False\n",
    "division            __future__._Feature           _Feature((2, 2, 0, 'alpha<...> 0, 0, 'alpha', 0), 8192)\n",
    "doubleQ             ndarray                       32: 32 elems, type `float32`, 128 bytes\n",
    "e                   float                         0.91252\n",
    "endE                float                         0.1\n",
    "end_multiplier      ndarray                       32: 32 elems, type `object`, 256 bytes\n",
    "env                 gridworld.gameEnv             <gridworld.gameEnv instance at 0x7f14d068bcb0>\n",
    "episodeBuffer       __main__.experience_buffer    <__main__.experience_buff<...>stance at 0x7f147489be18>\n",
    "experience_buffer   classobj                      __main__.experience_buffer\n",
    "gameEnv             classobj                      gridworld.gameEnv\n",
    "gym                 module                        <module 'gym' from '/home<...>ckages/gym/__init__.pyc'>\n",
    "h_size              int                           512\n",
    "i                   int                           219\n",
    "init                Operation                     name: \"init\"\\nop: \"NoOp\"\\<...>riable_3/Adam_1/Assign\"\\n\n",
    "j                   int                           22\n",
    "jList               list                          n=219\n",
    "load_model          bool                          False\n",
    "mainQN              __main__.Qnetwork             <__main__.Qnetwork instance at 0x7f14750ffbd8>\n",
    "max_epLength        int                           50\n",
    "myBuffer            __main__.experience_buffer    <__main__.experience_buff<...>stance at 0x7f1474891e60>\n",
    "np                  module                        <module 'numpy' from '/ho<...>ages/numpy/__init__.pyc'>\n",
    "num_episodes        int                           10000\n",
    "os                  module                        <module 'os' from '/home/<...>ow/lib/python2.7/os.pyc'>\n",
    "path                str                           ./dqn\n",
    "plt                 module                        <module 'matplotlib.pyplo<...>s/matplotlib/pyplot.pyc'>\n",
    "pre_train_steps     int                           10000\n",
    "processState        function                      <function processState at 0x7f14751aa5f0>\n",
    "r                   float                         0.0\n",
    "rAll                float                         0.0\n",
    "rList               list                          n=219\n",
    "random              module                        <module 'random' from '/h<...>ib/python2.7/random.pyc'>\n",
    "s                   ndarray                       21168: 21168 elems, type `uint8`, 21168 bytes\n",
    "s1                  ndarray                       21168: 21168 elems, type `uint8`, 21168 bytes\n",
    "saver               Saver                         <tensorflow.python.traini<...>object at 0x7f1474bbdf10>\n",
    "scipy               module                        <module 'scipy' from '/ho<...>ages/scipy/__init__.pyc'>\n",
    "sess                Session                       <tensorflow.python.client<...>object at 0x7f1475005f90>\n",
    "slim                module                        <module 'tensorflow.contr<...>ntrib/slim/__init__.pyc'>\n",
    "startE              int                           1\n",
    "stepDrop            float                         9e-05\n",
    "targetOps           list                          n=6\n",
    "targetQ             ndarray                       32: 32 elems, type `object`, 256 bytes\n",
    "targetQN            __main__.Qnetwork             <__main__.Qnetwork instance at 0x7f1474c1c4d0>\n",
    "tau                 float                         0.001\n",
    "tf                  module                        <module 'tensorflow' from<...>tensorflow/__init__.pyc'>\n",
    "total_steps         int                           10972\n",
    "trainBatch          ndarray                       32x5: 160 elems, type `object`, 1280 bytes\n",
    "trainables          list                          n=12\n",
    "updateTarget        function                      <function updateTarget at 0x7f14750250c8>\n",
    "updateTargetGraph   function                      <function updateTargetGraph at 0x7f147503d500>\n",
    "update_freq         int                           4\n",
    "y                   float                         0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
