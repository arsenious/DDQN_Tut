{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(tf.add(40,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVCw2uQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f01e2e9ac10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load game env\n",
    "\n",
    "from gridworld import gameEnv\n",
    "env = gameEnv(partial=False, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self.h_size):\n",
    "        #h_size is number of output filters at the end of the final conv layer\n",
    "        #In a regular DQN, this would just get flattened to get the Q-values for all actions\n",
    "        #But here these output filter maps are going to be sent to the V and A networks\n",
    "        \n",
    "        #We get frame, flatten it using def processState(states) and then reshape it here\n",
    "        \n",
    "        self.scalarInput = tf.placeholder(shape=[None, 84*84*3], dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1, 84, 84, 3])\n",
    "        \n",
    "        #CNN layers\n",
    "        \n",
    "        self.conv1 = slim.conv2d( \n",
    "            inputs=self.imageIn,\n",
    "            num_outputs=32,\n",
    "            kernel_size=[8,8],\n",
    "            stride=[4,4],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        self.conv2 = slim.conv2d( \n",
    "            inputs=self.conv1,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[4,4],\n",
    "            stride=[2,2],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )        \n",
    "        \n",
    "        self.conv3 = slim.conv2d( \n",
    "            inputs=self.conv2,\n",
    "            num_outputs=64,\n",
    "            kernel_size=[3,3],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        self.conv4 = slim.conv2d( \n",
    "            inputs=self.conv3,\n",
    "            num_outputs=h_size,\n",
    "            kernel_size=[7,7],\n",
    "            stride=[1,1],\n",
    "            padding='VALID',\n",
    "            biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        # In a regular DQN we'd flatten now and use Fully Connected layers\n",
    "        # But here we split these feature maps in two streams and then flatten them \n",
    "        # Could we have flattened and then split?\n",
    "        \n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3) #Split conv4 output into 2 parts, along last axis\n",
    "        # Last axis here no longer denotes color \n",
    "        \n",
    "        #Flatten\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        \n",
    "        #We put both A and V streams through 1 fully connected layer\n",
    "        #So we need weight matrices for both\n",
    "        #Random normal initialization\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2, env.actions])) \n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        #Advantage values will be 1 per action based on the state\n",
    "        #But just 1 V value for the state\n",
    "        \n",
    "        \n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
